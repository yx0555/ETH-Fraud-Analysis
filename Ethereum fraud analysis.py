# -*- coding: utf-8 -*-
"""Copy of Data Inspection & Cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HnVE3VtYtY_-KRygqg90gNf0kB8as7KF

**BT4012 Group 12**
<br> In this notebook, we will be exploring the [Ethereum dataset](https://www.kaggle.com/datasets/vagifa/ethereum-frauddetection-dataset) (Section A) and also the machine learning models (Section B) that we used for our Fraud Analytics Project.

*More elaborations and explanation can be found in our report.*

# **A. Pre-processing Steps**

In this section, we aim to get an overview of the dataset (number of observations, number of variables, type of variables, etc.). This will help us to better understand our dataset even before we carry out any form of fraud analysis. We also intend to identify any abnormalities that can potentially lead to biases in our analysis.

## **1. Load Data**
"""

# Import packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency
import seaborn as sns
from sklearn.model_selection import train_test_split

url = 'https://raw.githubusercontent.com/joeyylow/EthereumFraudAnalysis/main/transaction_dataset.csv'
df = pd.read_csv(url, index_col=0)

from warnings import filterwarnings
filterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')

"""## **2. Exploratory Data Analysis (EDA)**

### **2.1 Overview of each column in our dataset**
"""

# Displaying the first 5 rows of values for each column
df.head()

"""### **2.2 Hypotheses**

#### 2.2.1 Hypothesis 1: **Our dataset is imbalanced**
"""

# Calculating % of non-fraud vs fraud activities
flag_count = df['FLAG'].value_counts()
print(flag_count)
#print('\n')

non_fraud = df['FLAG'].where(df['FLAG'] == 0)
no_of_non_fraud = non_fraud.count()
fraud = df['FLAG'].where(df['FLAG'] == 1)
no_of_fraud = fraud.count()
total_flags = df['FLAG'].count()
percentage_non_fraud = round((no_of_non_fraud/total_flags)*100, 2)
percentage_fraud = round((no_of_fraud/total_flags)*100, 2)

print(f'The percentage of Non-Fraud activities is {percentage_non_fraud}%.')
print(f'The percentage of Fraudulent activities is {percentage_fraud}%.')
print('\n')

# Plotting a stacked bar graph
ax = (df.groupby('FLAG')[['FLAG']].count()/df[['FLAG']].count()).T.plot(
    kind = 'barh',
    stacked = True,
    width = 0.3,
    title = 'Proportion of Non-Fraud vs Fraud Transactions'
)

"""#### 2.2.2 Hypothesis 2: **Each row in our dataset is unique**"""

# Displaying all the duplicated rows
df[df.iloc[: , 1:].duplicated()]

"""#### 2.2.3 Hypothesis 3: **Each row represents one transaction**"""

df.head()

"""After taking a closer look at each column, it did not make sense to us that each row is a transaction. Our hypothesis was proven wrong and we now believe that each row represents the **activities of a particular address** (value in `Address` column), instead of representing one transaction.

#### 2.2.4 Hypothesis 4: **There are missing values in our dataset**
"""

# White portions of heatmap suggest the presence of missing values
plt.figure(figsize=(12,6))
sns.heatmap(df.isnull(), cbar = False)
plt.show()

# Number of null values in each column
df.isnull().sum()

# Extract the columns with missing values
# PART 1: Only extracting the columns with the same number of missing values 

missing_values_columns = df.iloc[:,25:48]
na_columns = missing_values_columns[missing_values_columns[' Total ERC20 tnxs'].isna()]
na_columns

na_columns.count()

"""The above shows that the rows that have missing values for each of these columns are the same rows across all columns."""

# Observing the `flag` and the columns with NA values
na_dataframe1 = df[df[' Total ERC20 tnxs'].isna()]
na_df1 = na_dataframe1['FLAG']
na_df2 = na_dataframe1.iloc[:,25:48]
newdf1 = pd.concat([na_df1, na_df2], axis=1)
newdf1

# Verifying if the rows with NA values for columns 25 and onwards have their `flag` values as 1
newdf1[newdf1['FLAG'] == 0]

"""There are no rows (transactions) displayed above. This suggests that all the rows with missing values for columns 25 onwards are considered as fraudulent transactions."""

percentage = na_dataframe1['FLAG'].sum()/df['FLAG'].sum()*100
round(percentage, 2)
print('The missing values in columns 25 onwards make up ' + str(round(percentage, 2)) + '% of all fraudulent transactions in the dataset.')

"""This significant number of fraudulent transactions reminds us that we need to exercise caution when deciding what to do with the missing values later on during data cleaning. Deletion of the rows with missing values might not be the most ideal."""

# PART 2: Extracting columns with more than 829 missing values (`erc20 most sent token type` and `erc20_most_rec_token_type`)

# `ERC20 most sent token type`
na_dataframe2 = df[df[' ERC20 most sent token type'].isna()]
na_df2a = na_dataframe2['FLAG']
na_df2b = na_dataframe2[' ERC20 most sent token type']
newdf2 = pd.concat([na_df2a, na_df2b], axis=1)

no_of_fraud = newdf2['FLAG'].sum()
no_of_na = df[' ERC20 most sent token type'].isnull().sum()
percentage_of_fraud = no_of_fraud/no_of_na*100
round(percentage_of_fraud, 2)
print(str(round(percentage_of_fraud, 2)) + '% of the NA values in `ERC20 most sent token type` are fraudulent transactions.')

# `ERC20_most_rec_token_type`
na_dataframe3 = df[df[' ERC20_most_rec_token_type'].isna()]
na_df3a = na_dataframe3['FLAG']
na_df3b = na_dataframe3[' ERC20_most_rec_token_type']
newdf3 = pd.concat([na_df3a, na_df3b], axis=1)

no_of_fraud = newdf3['FLAG'].sum()
no_of_na = df[' ERC20_most_rec_token_type'].isnull().sum()
percentage_of_fraud = no_of_fraud/no_of_na*100
print(str(round(percentage_of_fraud, 2)) + '% of the NA values in `ERC20_most_rec_token_type` are fraudulent transactions.')

"""As seen from *Section 2.2.1 Hypothesis 1*, the percentage of Non-Fraud transactions is 77.86%, while the percentage of Fraudulent transactions is 22.14%. However, for both columns `ERC20 most sent token type` and `ERC20_most_rec_token_type`, we noticed that 98.57% and 97.41% of the NA values in `ERC20 most sent token type` and `ERC20_most_rec_token_type` are fraudulent transactions respectively. 

These much higher weightages of the fraudulent transactions of these NA values (98.57% and 97.41%)  as compared to that of the weightage of the overall dataset (22.14%) shows that there could be some correlation between the labels and the columns.

Hence this reminds us that deletion of these columns might not be the most ideal. We could explore other methods of handling missing values in the data cleaning section.

#### 2.2.5 Hypothesis 5: **Our data is skewed**

##### A. Checking skewness using `.skew()`
"""

df.select_dtypes(include = ['float64', 'int64']).skew()

"""##### B. Investigating the distribution of our features using boxplots."""

columns = df.columns
columns

# Plotting the boxplots
fig, axes = plt.subplots(12, 4, figsize=(15, 15))
fig.delaxes(axes[11][2])
fig.delaxes(axes[11][3])
plt.subplots_adjust(wspace=0.25, hspace=1.5)
plt.suptitle("Distribution of Features",y=0.92, size=22, weight='bold')

ax1 = sns.boxplot(ax = axes[0,0], data=df, x=columns[2])
ax2 = sns.boxplot(ax = axes[0,1], data=df, x=columns[3])
ax3 = sns.boxplot(ax = axes[0,2], data=df, x=columns[4])
ax4 = sns.boxplot(ax = axes[0,3], data=df, x=columns[5])
ax5 = sns.boxplot(ax = axes[1,0], data=df, x=columns[6])
ax6 = sns.boxplot(ax = axes[1,1], data=df, x=columns[7])
ax7 = sns.boxplot(ax = axes[1,2], data=df, x=columns[8])
ax8 = sns.boxplot(ax = axes[1,3], data=df, x=columns[9])
ax9 = sns.boxplot(ax = axes[2,0], data=df, x=columns[10])
ax10 = sns.boxplot(ax = axes[2,1], data=df, x=columns[11])
ax11 = sns.boxplot(ax = axes[2,2], data=df, x=columns[12])
ax12 = sns.boxplot(ax = axes[2,3], data=df, x=columns[13])
ax13 = sns.boxplot(ax = axes[3,0], data=df, x=columns[14])
ax14 = sns.boxplot(ax = axes[3,1], data=df, x=columns[15])
ax15 = sns.boxplot(ax = axes[3,2], data=df, x=columns[16])
ax16 = sns.boxplot(ax = axes[3,3], data=df, x=columns[17])
ax17 = sns.boxplot(ax = axes[4,0], data=df, x=columns[18])
ax18 = sns.boxplot(ax = axes[4,1], data=df, x=columns[19])
ax19 = sns.boxplot(ax = axes[4,2], data=df, x=columns[20])
ax20 = sns.boxplot(ax = axes[4,3], data=df, x=columns[21])
ax21 = sns.boxplot(ax = axes[5,0], data=df, x=columns[22])
ax22 = sns.boxplot(ax = axes[5,1], data=df, x=columns[23])
ax23 = sns.boxplot(ax = axes[5,2], data=df, x=columns[24])
ax24 = sns.boxplot(ax = axes[5,3], data=df, x=columns[25])
ax25 = sns.boxplot(ax = axes[6,0], data=df, x=columns[26])
ax26 = sns.boxplot(ax = axes[6,1], data=df, x=columns[27])
ax27 = sns.boxplot(ax = axes[6,2], data=df, x=columns[28])
ax28 = sns.boxplot(ax = axes[6,3], data=df, x=columns[29])
ax29 = sns.boxplot(ax = axes[7,0], data=df, x=columns[30])
ax30 = sns.boxplot(ax = axes[7,1], data=df, x=columns[31])
ax31 = sns.boxplot(ax = axes[7,2], data=df, x=columns[32])
ax32 = sns.boxplot(ax = axes[7,3], data=df, x=columns[33])
ax33 = sns.boxplot(ax = axes[8,0], data=df, x=columns[34])
ax34 = sns.boxplot(ax = axes[8,1], data=df, x=columns[35])
ax35 = sns.boxplot(ax = axes[8,2], data=df, x=columns[36])
ax36 = sns.boxplot(ax = axes[8,3], data=df, x=columns[37])
ax37 = sns.boxplot(ax = axes[9,0], data=df, x=columns[38])
ax38 = sns.boxplot(ax = axes[9,1], data=df, x=columns[39])
ax39 = sns.boxplot(ax = axes[9,2], data=df, x=columns[40])
ax40 = sns.boxplot(ax = axes[9,3], data=df, x=columns[41])
ax41 = sns.boxplot(ax = axes[10,0], data=df, x=columns[42])
ax42 = sns.boxplot(ax = axes[10,1], data=df, x=columns[43])
ax43 = sns.boxplot(ax = axes[10,2], data=df, x=columns[44])
ax44 = sns.boxplot(ax = axes[10,3], data=df, x=columns[45])
ax45 = sns.boxplot(ax = axes[11,0], data=df, x=columns[46])
ax46 = sns.boxplot(ax = axes[11,1], data=df, x=columns[47])

plt.show()

"""#### 2.2.6 Hypothesis 6: **Our dataset has a mix of numerical and categorical type**"""

df.info()

"""From the list displayed above, for the columns which we thought were numerical, we learnt that they have specific types - some of them are of 'int' type, while the rest are of 'float64' type.

For the three columns which we thought were categorical, they are actually of type 'object'.

#### 2.2.7 Hypothesis 7: **Variables of type 'category' have better memory optimization than variables of type 'object'**

After discovering that the variables we thought were of type 'category' were actually of type 'object', we wanted to find which type would allow for better memory optimization, which thereby enables higher computational efficiency when we run our model.
"""

# Memory usage for variables of type 'object'
address_old_memory = df['Address'].memory_usage(deep=True)
erc20sent_old_memory = df[' ERC20 most sent token type'].memory_usage(deep=True)
erc20rec_old_memory = df[' ERC20_most_rec_token_type'].memory_usage(deep=True)

print(f'The memory usage of `Address` when it is of object type is {address_old_memory}.')
print(f'The memory usage of `ERC20 most sent token type` when it is of object type is {erc20sent_old_memory}.')
print(f'The memory usage of `ERC20_most_rec_token_type` when it is of object type is {erc20rec_old_memory}.')

# Memory usage for variables of type 'category'
address_new_memory = df['Address'].astype('category').memory_usage(deep=True)
erc20sent_new_memory = df[' ERC20 most sent token type'].astype('category').memory_usage(deep=True)
erc20rec_new_memory = df[' ERC20_most_rec_token_type'].astype('category').memory_usage(deep=True)

print(f'The memory usage of `Address` when it is of category type is {address_new_memory}.')
print(f'The memory usage of `ERC20 most sent token type` when it is of category type is {erc20sent_new_memory}.')
print(f'The memory usage of `ERC20_most_rec_token_type` when it is of category type is {erc20rec_new_memory}.')

# Percentage difference of memory usage for 'object' vs 'category' type variables
address_percdiff = round(((address_new_memory-address_old_memory)/address_old_memory*100), 2)
erc20sent_percdiff = round(((erc20sent_new_memory-erc20sent_old_memory)/erc20sent_old_memory*100), 2)
erc20rec_percdiff = round(((erc20rec_new_memory-erc20rec_old_memory)/erc20rec_old_memory*100), 2)

print(f'The percentage difference of the memory usage for `Address` is {address_percdiff}%.')
print(f'The percentage difference of the memory usage for `ERC20 most sent token type` is {erc20sent_percdiff}%.')
print(f'The percentage difference of the memory usage for `ERC20_most_rec_token_type` is {erc20rec_percdiff}%.')

"""#### 2.2.8 Hypothesis 8: **Many repeated values for `ERC20 most sent token type` and `ERC20_most_rec_token_type`, but not for `Address`**"""

total_rows = df.shape[0]

erc20sent_unique = df[' ERC20 most sent token type'].nunique()
erc20sent_unique_perc = round((erc20sent_unique/total_rows*100), 2)
print(f'The percentage of unique values for `ERC20 most sent token type` is {erc20sent_unique_perc}%.')
print('\n')

df[' ERC20 most sent token type'].value_counts()

erc20rec_unique = df[' ERC20_most_rec_token_type'].nunique()
erc20rec_unique_perc = round((erc20rec_unique/total_rows*100), 2)
print(f'The percentage of unique values for `ERC20_most_rec_token_type` is {erc20rec_unique_perc}%.')
print('\n')

df[' ERC20_most_rec_token_type'].value_counts()

address_unique = df['Address'].nunique()
address_unique_perc = round((address_unique/total_rows*100), 2)
print(f'The percentage of unique values for `Address` is {address_unique_perc}%.')
print('\n')

df['Address'].value_counts()

"""## **3. Data Cleaning**

### 3.1 Making Column Names in Lowercase
"""

df.columns = [x.lower() for x in df.columns]

"""### 3.2 Removing Leading Spaces in Column Names"""

df.columns = df.columns.str.lstrip()

"""### 3.3 Omitting the `index` column"""

df = df.iloc[: , 1:]

print(df.shape)
df.head()

"""### 3.4 Removing Duplicated Rows"""

df.drop_duplicates(inplace=True)
df

"""### 3.5 Dealing with Missing Values

#### PART 1 ANALYSIS

From *Section 2.2.4 Hypothesis 4 Part 1*, we learnt that all rows that have missing values in columns 25 onwards are labelled as fraudulent transactions. This is a case of Missing data At Random (MAR). The missing values in columns 25 onwards make up 38.04% of all fraudulent transactions in the dataset.

Deletion of these rows with missing values is therefore not ideal as it can potentially bias data, whereby we delete a significant number of fraudulent transactions.

Since deletion of rows is not wise, we explored the possibility of column deletion. We checked the correlation coefficient of the `flag` column with the columns with missing values.
"""

# Correlation between `flag` and the columns with missing values
df1 = df['flag']
df2 = df.iloc[:,24:47]
newdf = pd.concat([df1, df2], axis=1)

newdf.corrwith(newdf['flag'])

"""None of the columns with missing values are strongly correlated with the label (`flag`)."""

# Removing the columns with missing values
drop_columns_with_missing_values = ['total erc20 tnxs', 'erc20 total ether received', 'erc20 total ether sent', 'erc20 total ether sent contract', 
                                    'erc20 uniq sent addr', 'erc20 uniq rec addr', 'erc20 uniq sent addr.1', 'erc20 uniq rec contract addr',
                                    'erc20 avg time between sent tnx', 'erc20 avg time between rec tnx', 'erc20 avg time between rec 2 tnx',
                                    'erc20 avg time between contract tnx', 'erc20 min val rec', 'erc20 max val rec', 'erc20 avg val rec',
                                    'erc20 min val sent', 'erc20 max val sent', 'erc20 avg val sent', 
                                    'erc20 min val sent contract', 'erc20 max val sent contract', 'erc20 avg val sent contract',
                                    'erc20 uniq sent token name', 'erc20 uniq rec token name']
df.drop(drop_columns_with_missing_values, axis=1, inplace=True)

plt.figure(figsize=(12,6))
sns.heatmap(df.isnull(), cbar = False)
plt.show()

"""There are still some white portions for the columns `erc20 most sent token type` and `erc20_most_rec_token_type`. These are actually categorical variables, as seen from *Section 2.2.6 Hypothesis 6*.

#### PART 2 ANALYSIS

From *Section 2.2.4 Hypothesis 4 Part 2*, we learnt that there could be some correlation between these columns and the labels. We verified the presence of correlation using the Chi-Square Test in the first code block.

Hence we should not delete the columns as that might cause the model to perform not as well. Thus, we chose imputation of values as our method of dealing with missing values in these two columns.

We are also aware that imputing the modal value can make the data imbalanced if there are a huge number of missing values present in our dataset. However, we have verified that only 9.36% and 9.49% of the values in `erc20 most sent token type` and `erc20_most_rec_token_type` respectively are missing, as shown by the second and third code blocks below.
"""

# Finding correlation between the two column and `flag` using Chi-Square Test
dfcopy = df[['flag']].copy()
dfcopy[['flag']] = dfcopy[['flag']].astype('category')

contigency = pd.crosstab(df['erc20 most sent token type'], dfcopy['flag'])
c, p, dof, expected = chi2_contingency(contigency)
pvalue = round(p, 5)
print(f'The p-value of the Chi-Square Test for `erc20 most sent token type` and `flag` is {pvalue}.')

contigency = pd.crosstab(df['erc20_most_rec_token_type'], dfcopy['flag'])
c, p, dof, expected = chi2_contingency(contigency)
pvalue = round(p, 5)
print(f'The p-value of the Chi-Square Test for `erc20_most_rec_token_type` and `flag` is {pvalue}.')

"""The null hypothesis of the Chi-Square Test is that `erc20 most sent token type`/`erc20_most_rec_token_type` and `flag` are independent. Since the p-values for both Chi-Square Test is 0.0, we have sufficient evidence to reject the null hypothesis at 5% level of significance. Thus we say that both `erc20 most sent token type` and `erc20_most_rec_token_type` are correlated with `flag`."""

# Percentage of missing data in `erc20 most sent token type`
no_of_na = df['erc20 most sent token type'].isnull().sum()
total_no_of_data = df['erc20 most sent token type'].count()
percentage_missing_data = no_of_na/total_no_of_data*100
print(str(round(percentage_missing_data, 2)) + '% of the values in `erc20 most sent token type` are NA values.')

# Percentage of missing data in `erc20_most_rec_token_type`
no_of_na = df['erc20_most_rec_token_type'].isnull().sum()
total_no_of_data = df['erc20_most_rec_token_type'].count()
percentage_missing_data = no_of_na/total_no_of_data*100
print(str(round(percentage_missing_data, 2)) + '% of the values in `erc20_most_rec_token_type` are NA values.')

"""Thus using the method of imputation to deal with the missing values will not result in an imbalanced dataset."""

# Replace missing categorical variables with the category of the highest frequency
mode_for_most_cat = df['erc20 most sent token type'].mode()[0]
mode_for_rec_cat = df['erc20_most_rec_token_type'].mode()[0]

df['erc20 most sent token type'].fillna(mode_for_most_cat, inplace=True)
df['erc20_most_rec_token_type'].fillna(mode_for_rec_cat, inplace=True)

df['erc20 most sent token type'].replace('\s+', mode_for_most_cat, regex=True, inplace=True)
df['erc20_most_rec_token_type'].replace('\s+', mode_for_rec_cat, regex=True, inplace=True)

plt.figure(figsize=(12,6))
sns.heatmap(df.isnull(), cbar = False)
plt.show()

"""The above heatmap shows that there are no longer any missing values in our dataset.

### 3.6 Changing selected 'object' variables to 'category' type

From *Section 2.2.7 Hypothesis 7*, we deduced that only columns with many repeated values (`erc20 most sent token type` and `erc20_most_rec_token_type`) led to a decrease in memory usage when changed from 'object' to 'category' type.

Conversely, changing the `address` column from 'object' type to 'category' type led to an increase in memory usage as there are very few repeated values in the `address` column.

Hence we would only be changing the type of columns `erc20 most sent token type` and `erc20_most_rec_token_type` from 'object' to 'category' for better memory optimization and computational efficiency, we decided to change the object variables into 'category' Dtype.
"""

df[['erc20 most sent token type', 'erc20_most_rec_token_type']] = df[['erc20 most sent token type', 'erc20_most_rec_token_type']].astype('category')
df.info()

"""We notice a decrease in the memory usage of the dataset. The memory usage decreased from 3.8+ MB (as seen in *Section 2.2.6 Hypothesis 6*) to the current 2.2+ MB after converting `erc20 most sent token type` and `erc20_most_rec_token_type` columns to be of type 'category'.

### 3.7 Removing ‘object’/ ‘category’ Variables

To determine whether we should keep the three categorical variables (`address`, `erc20 most sent token type` and `erc20_most_rec_token_type`) or not, we looked further into each of the columns.
"""

df['erc20 most sent token type'].value_counts()

df['erc20_most_rec_token_type'].value_counts()

"""For `erc20 most sent token type` and `erc20_most_rec_token_type`, we noticed that more than half of all the rows in the two columns is made up of the value '0'.

Based on our domain knowledge, these two columns are supposed to have values that represent actual names of the token. The value '0' does not mean anything in these two categorical columns.

Hence we will drop these two columns.
"""

# Dropping `erc20 most sent token type` and `erc20_most_rec_token_type` columns
df.drop(columns=['erc20 most sent token type', 'erc20_most_rec_token_type'], inplace = True)

"""On the other hand, the `address` column has very few repeated values. Most of its values are unique.



"""

df['address'].value_counts()

"""Therefore, we will use the correlation between the `address` column and `flag` label to determine whether we should keep this variable or not."""

# Chi-Square Test
contigency = pd.crosstab(df['address'], dfcopy['flag'])
c, p, dof, expected = chi2_contingency(contigency)
pvalue = round(p, 5)
print(f'The p-value of the Chi-Square Test is {pvalue}.')

"""The null hypothesis of the Chi-Square Test is that `address` and `flag` are independent. 

Since the p-value of our Chi-Square Test is 0.47534, we do not have sufficient evidence to reject the null hypothesis at 5% level of significance. This means that the `address` column and `flag` label are not correlated to each other. 

Thus we will drop the `address` column.
"""

df.drop(columns=['address'], inplace = True)

"""## **4. Splitting Data into Training, Validation & Test Sets**"""

from imblearn.under_sampling import EditedNearestNeighbours
from collections import Counter

undersample = EditedNearestNeighbours(n_neighbors=3)

train_size=0.8
features = [x for x in df.columns if (x != 'flag')]
unique_values = df.nunique()
features = [x for x in features if x in unique_values.loc[(unique_values>1)]]
X = df[features]
y = df['flag']
counter = Counter(y)
print(counter)
X, y = undersample.fit_resample(X,y)
counter = Counter(y)
print(counter)
# Split the data into training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size = 0.8)

# Split the remaining dataset into validation and test sets
test_size = 0.5
X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size = 0.5)

df[features].info()

# Check if the distribution of the test dataset follows the overall distribution of the entire dataset

# total_test_flags = y_test.count()
# no_of_test_fraud = y_test.sum()
# no_of_test_nonfraud = total_test_flags - no_of_test_fraud
# percentage_test_fraud = round((no_of_test_fraud/total_test_flags*100), 2)
# percentage_test_nonfraud = round((no_of_test_nonfraud/total_test_flags*100), 2)

# print(f'The percentage of Non-Fraud transactions in our test set is {percentage_test_nonfraud}%.')
# print(f'The percentage of Fraudulent transactions in our test set is {percentage_test_fraud}%.')

"""## **5. Balancing the Imbalanced Dataset**"""

from imblearn import *
from collections import Counter
from numpy import where
from matplotlib import pyplot
from imblearn.under_sampling import EditedNearestNeighbours

# NEED TO REARRANGE, RESAMPLED BEFORE TRAIN TEST VALIDATION SPLIT

# undersample = EditedNearestNeighbours(n_neighbors=3)


# oversample = over_sampling.SMOTE()
# X_train, y_train = oversample.fit_resample(X_train, y_train)

counter = Counter(y_train)
print(counter)

X_train

"""## **6. Feature Scaling**"""

from sklearn import preprocessing
import numpy as np
# from feature_engine import transformation as vt
from scipy.special import boxcox1p
from sklearn.preprocessing import PowerTransformer


pt = PowerTransformer(method='yeo-johnson')
X_train_robust = pt.fit_transform(X_train)
# robust_scaler = preprocessing.RobustScaler()
# X_train_robust = robust_scaler.fit_transform(X_train)

# tf = vt.BoxCoxTransformer(variables = list(X_train.columns))
# X_train_robust = pd.DataFrame()
# for col in X_train:
#   # print(X_train[col])
#   # print(len([num for num in X_train[col] if num < 0]))
#   X_train_robust[col], fitted_lambda = stats.boxcox(X_train[col])

columns = X_train.columns
X_train_robust = pd.DataFrame(data=X_train_robust, columns=columns)


fig, axes = plt.subplots(7, 3, figsize=(15, 15), constrained_layout =True)

plt.subplots_adjust(wspace = 0.7, hspace=0.8)
plt.suptitle("Distribution of features",y=0.95, size=18, weight='bold')

ax1 = sns.boxplot(ax = axes[0,0], data=X_train_robust, x=columns[2])
ax1.set_title(f'Distribution of {columns[2]}')

ax2 = sns.boxplot(ax = axes[0,1], data=X_train_robust, x=columns[3])
ax2.set_title(f'Distribution of {columns[3]}')

ax3 = sns.boxplot(ax = axes[0,2], data=X_train_robust, x=columns[4])
ax3.set_title(f'Distribution of {columns[4]}')

ax4 = sns.boxplot(ax = axes[1,0], data=X_train_robust, x=columns[5])
ax4.set_title(f'Distribution of {columns[5]}')

ax5 = sns.boxplot(ax = axes[1,1], data=X_train_robust, x=columns[6])
ax5.set_title(f'Distribution of {columns[6]}')

ax6 = sns.boxplot(ax = axes[1,2], data=X_train_robust, x=columns[7])
ax6.set_title(f'Distribution of {columns[7]}')

ax7 = sns.boxplot(ax = axes[2,0], data=X_train_robust, x=columns[8])
ax7.set_title(f'Distribution of {columns[8]}')

ax8 = sns.boxplot(ax = axes[2,1], data=X_train_robust, x=columns[9])
ax8.set_title(f'Distribution of {columns[9]}')

ax9 = sns.boxplot(ax = axes[2,2], data=X_train_robust, x=columns[10])
ax9.set_title(f'Distribution of {columns[10]}')
 
ax10 = sns.boxplot(ax = axes[3,0], data=X_train_robust, x=columns[11])
ax10.set_title(f'Distribution of {columns[11]}')

ax11 = sns.boxplot(ax = axes[3,1], data=X_train_robust, x=columns[12])
ax11.set_title(f'Distribution of {columns[12]}')
 
ax12 = sns.boxplot(ax = axes[3,2], data=X_train_robust, x=columns[13])
ax12.set_title(f'Distribution of {columns[13]}')
 
ax13 = sns.boxplot(ax = axes[4,0], data=X_train_robust, x=columns[14])
ax13.set_title(f'Distribution of {columns[14]}')
 
ax14 = sns.boxplot(ax = axes[4,1], data=X_train_robust, x=columns[15])
ax14.set_title(f'Distribution of {columns[15]}')
 
ax15 = sns.boxplot(ax = axes[4,2], data=X_train_robust, x=columns[16])
ax15.set_title(f'Distribution of {columns[16]}')
 
ax16 = sns.boxplot(ax = axes[5,0], data=X_train_robust, x=columns[17])
ax16.set_title(f'Distribution of {columns[17]}')
 
ax17 = sns.boxplot(ax = axes[5,1], data=X_train_robust, x=columns[18])
ax17.set_title(f'Distribution of {columns[18]}')

ax18 = sns.boxplot(ax = axes[5,2], data=X_train_robust, x=columns[19])
ax18.set_title(f'Distribution of {columns[19]}')

ax19 = sns.boxplot(ax = axes[6,0], data=X_train_robust, x=columns[20])
ax19.set_title(f'Distribution of {columns[20]}')

"""## **7. Feature Selection**

### 7.1 Correlation between Features

We created a correlation matrix to determine the correlation between the features in our dataset.
"""

X_train_robust.corr()

corr = X_train_robust.corr()
mask = np.zeros_like(corr)
mask[np.triu_indices_from(mask)] = True
with sns.axes_style('white'):
    fig, ax = plt.subplots(figsize = (15, 8))
    sns.heatmap(corr,  mask = mask, annot = False, cmap = 'CMRmap', center = 0, square = True)

"""Based on the correlation matrix, we noticed that there are several variables that are very highly correlated with each other. We define 'very highly correlated' as correlation coefficients that are between 0.9 and 1.0 (inclusive)."""

corr_mat = X_train_robust.corr(method='pearson')
upper_corr_mat = corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))
unique_corr_pairs = upper_corr_mat.unstack().dropna()
abs_unique_corr_pairs = abs(unique_corr_pairs)
sorted_mat = abs_unique_corr_pairs.sort_values()
print(sorted_mat)

# Extracting very highly correlated variables
sorted_mat[sorted_mat > 0.9]

# Removing variables that are very highly correlated with each other
highly_related_variables = ['avg value sent to contract', 'max val sent to contract', 'total ether sent contracts']
df.drop(highly_related_variables, axis=1, inplace=True)

# X_train.drop(highly_related_variables, axis=1, inplace=True)
# y_train.drop(highly_related_variables, inplace=True)

# Recheck the correlation matrix
new_corr = X_train.corr()

mask = np.zeros_like(new_corr)
mask[np.triu_indices_from(mask)]=True
with sns.axes_style('white'):
    fig, ax = plt.subplots(figsize=(18,10))
    sns.heatmap(new_corr,  mask=mask, annot=False, cmap='CMRmap', center=0, linewidths=0.1, square=True)

"""### 7.2 Removing Features with a Small Distribution"""

# Features that have a small distribution
for i in df.columns[2:]:
    if len(df[i].value_counts()) < 10:
        print(f'The column `{i}` has the following distribution: \n{df[i].value_counts()}')
        print('======================================')

"""The values in the column `min value sent to contract` are mostly zeroes. This feature will not be useful for our model."""

# Removing the feature with a small distribution
small_distri_column = ['min value sent to contract']
df.drop(small_distri_column, axis=1, inplace=True)
print(df.shape)
df.head()

"""# **B. Fraud Analytics Models**

## **1. Logistic Regression**
"""

from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix 
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

class LogisticRegressionEstimator: 
  def __init__(self, x_train, y_train, x_validation, y_validation, x_test, y_test): 
    self.x_train = x_train
    self.y_train = y_train
    self.x_validation = x_validation
    self.y_validation = y_validation
    self.x_test = x_test
    self.y_test = y_test

  def fit_model(self): 
    """
    Returns fitted logistic regression model object
    """
    lr = LogisticRegression()
    lr.fit(self.x_train, self.y_train)
    return lr

  def display_model_performance(self, test_type, output_training=False): 
    """
    Displays confusion matrix, AUC, Accuracy, Precision and recall
    after testing model on validation or testing dataset

    Parameters
    ==========
    model
      Fitted logistic regression model
    test_type
      Specify whether to use validation or test dataset
    """
    # Fit model 
    model = self.fit_model()
    y_train = self.y_train
    x_train = self.x_train

    #Training Score
    y_train_hat = model.predict(x_train)
    y_train_hat_probs = model.predict_proba(x_train)[:,1]

    train_accuracy = accuracy_score(y_train, y_train_hat)*100
    train_auc_roc = roc_auc_score(y_train, y_train_hat_probs)*100

    #confusion matrix shows true positives, false positives, false negatives, and true negatives
    print('Training Confusion matrix:\n', confusion_matrix(y_train, y_train_hat))

    #fetching the area under the receiver-operator-curve for the model that we have built
    print('Training AUC: %.4f %%' % train_auc_roc)

    #to note that accuracy score can be misleading for imbalanced dataset
    print('Training accuracy: %.4f %%' % train_accuracy)

    print('Training Classification report:\n', classification_report(y_train, y_train_hat, digits=6)) 

    #Model validation
    x_test = self.x_validation if test_type == 'validation' else self.x_test
    y_test = self.y_validation if test_type == 'validation' else self.y_test

    y_test_hat = model.predict(x_test)
    y_test_probs = model.predict_proba(x_test)[:,1]

    test_accuracy = accuracy_score(y_test, y_test_hat)*100
    test_auc_roc = roc_auc_score(y_test, y_test_probs)*100


    print('Testing Confusion matrix:\n', confusion_matrix(y_test, y_test_hat))

    print('Testing AUC: %.4f %%' % test_auc_roc)

    print('Testing accuracy: %.4f %%' % test_accuracy) 

    print('Testing Classification report:\n', classification_report(y_test, y_test_hat, digits=6)) 

    logit_roc_auc = roc_auc_score(y_test, model.predict(x_test))
    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test)[:,1])
    plt.figure()
    plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.savefig('Log_ROC')
    plt.show()

"""We first train our baseline model which will be used as a benchmark to compare the performance of more complex models. <br>

Our baseline model is a logistic regression model which uses all features from our training dataset after feature engineering was performed
"""

#todo use xrobust scaler as inputs
# lr_estimator = LogisticRegressionEstimator(x_train, y_train, x_validation, y_validation, x_test, y_test)
# lr_estimator.display_model_performance(test_type = "test")

"""## **2. XGBoost Algorithm**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# #Feature importance classification for Xgboost model
# 
# from xgboost import XGBClassifier
# from matplotlib import pyplot
# from xgboost import plot_importance
# from sklearn.metrics import precision_score
# from sklearn.metrics import recall_score
# 
# 
# xgb_model = XGBClassifier()
# xgb_model.fit(X_train_robust, y_train)
# plot_importance(xgb_model)
# pyplot.show()
# 
# 
# for i in range(1,17):
#   feature_importance = xgb_model.feature_importances_
#   sorted_idx = np.argsort(feature_importance)
#   topfeatures = sorted_idx[:i]
# 
#   #Get the features for the XG boost feature importance method
#   x_train_xgboost = X_train.iloc[:,topfeatures]
# 
#   #train the model
#   improved_xgb_model = XGBClassifier()
#   improved_xgb_model.fit(x_train_xgboost, y_train)
# 
#   y_pred = improved_xgb_model.predict(X_test.iloc[:,topfeatures])
#   # accuracy = accuracy_score(y_test, y_pred)
#   precision = precision_score(y_test, y_pred, average='binary')
#   print (i)
#   print('Precision for is {:.2f}'.format(precision))
#   recall = recall_score(y_test, y_pred, average='binary')
#   print('Recall is {:.2f}'.format(recall))
# 
#

# #Using Pearson's correlation analysis 
# from sklearn.datasets import make_regression
# from sklearn.feature_selection import SelectKBest
# from sklearn.feature_selection import f_regression

# sel_five_cols = SelectKBest(score_func=f_regression, k=10)
# sel_five_cols.fit(X_train, y_train)
# X_train.columns[sel_five_cols.get_support()]

# Numeric_df = pd.DataFrame(X_train.copy(deep=True))
# Numeric_df["flag"] = y_train
# plt.figure(figsize=(12,10))
# corr = Numeric_df.corr()
# sns.heatmap(corr, cmap=plt.cm.Reds)
# cor_target = abs(corr["flag"])

# cor_target = abs(corr["flag"])
# cor_target.sort_values(ascending=False)

"""## **3. Fully-Connected Neural Network**"""

import keras
import tensorflow
from keras.models import Sequential
from keras import layers
from sklearn.metrics import r2_score as r2

# number of hidden nodes
H = 100
# input dimension
input_dim = 8
# num of epochs
num_epochs = 100

# create sequential multi-layer perceptron
nn_model = Sequential()
# layer 0
nn_model.add(layers.Input(shape=(input_dim,))) 

# layer 1
nn_model.add(layers.Dense(H, activation='relu'))

#output layer
nn_model.add(layers.Dense(1, activation='sigmoid'))

# configure the model
nn_model.compile(loss='binary_crossentropy', optimizer='adam')
# fit the model 
nn_model.fit(x_train, y_train, batch_size=50, epochs=num_epochs, verbose=0)

# Tuning hyperparameters using cross validation

import numpy
from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier

def create_model(init_mode='uniform'):
    # define model
    # create sequential multi-layer perceptron
    nn_model = Sequential()
    # layer 0
    nn_model.add(layers.Input(shape=(input_dim,))) 

    # layer 1
    nn_model.add(layers.Dense(H, activation='relu'))

    #output layer
    nn_model.add(layers.Dense(1, activation='sigmoid'))

    # configure the model
    nn_model.compile(loss='binary_crossentropy', optimizer='adam')
    return nn_model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# seed = 7
# numpy.random.seed(seed)
# batch_size = 128
# epochs = 10
# 
# model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, 
#                            batch_size=batch_size, verbose=1)
# # define the grid search parameters
# init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 
#              'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']
# 
# param_grid = dict(init_mode=init_mode)
# grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)
# grid_result = grid.fit(x_train, y_train)

# print results
print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(f' mean={mean:.4}, std={stdev:.4} using {param}')

#Training Score
y_train_hat = nn_model.predict(x_train)

# evaluate the training and testing performance of your model 
train_score = nn_model.evaluate(x_train, y_train, verbose=0)
print('Train loss:', train_score)
print('Train R2:', r2(y_train, nn_model.predict(x_train)))

valid_score = nn_model.evaluate(x_validation, y_validation, verbose=0)
print('Validation loss:', valid_score)
print('Validation R2:', r2(y_validation, nn_model.predict(x_validation)))


#to change parameters to get higher R2